# ADR-001: Selection of AI-Based Automated Grading

## ğŸ“… Date
2025-02-16

## ğŸ¯ Status
âœ… Approved

## ğŸ“Œ Context
Manual grading of short-answer and case-study-based certification tests is time-consuming, inconsistent, and prone to biases. An AI-powered grading system is required to ensure scalability, fairness, and efficiency.

## ğŸ’¡ Decision
We will implement an AI-based grading engine using **Generative AI (GPT-4) and Transformer models (BERT, T5, RoBERTa)** to analyze responses. The system will include:
- **Text Preprocessing:** Normalizing responses by removing typos and irrelevant words.
- **Semantic Matching:** AI evaluates meaning using NLP, rather than exact words.
- **AI-Powered Rubric:** Assigns partial credit based on predefined scoring weightage.
- **Feedback Generation:** Personalized improvement suggestions from AI.

## ğŸ›  Technologies
- **NLP Models:** OpenAI GPT-4, Hugging Face Transformers (BERT, T5, RoBERTa)
- **Model Hosting:** Azure OpenAI / AWS SageMaker
- **Similarity Matching:** FAISS, SentenceTransformers

## ğŸš€ Consequences
âœ… Faster grading (~80-90% efficiency improvement).  
âœ… Ensures fairness and consistency.  
âœ… Reduces expert workload.  
â— Requires explainability mechanisms to ensure trust in AI-based grading.

## ğŸ“Œ Alternatives Considered
1ï¸âƒ£ **Traditional Rule-Based Grading** â€“ Inflexible and does not capture meaning. âŒ  
2ï¸âƒ£ **Fully Manual Grading** â€“ Not scalable for large volumes. âŒ  
3ï¸âƒ£ **Hybrid Approach** (AI + Human Review) â€“ Selected as the best approach. âœ…  

# ADR-002: Human-AI Hybrid Model for Complex Case Study Evaluations

## ğŸ“… Date
2025-02-16

## ğŸ¯ Status
âœ… Approved

## ğŸ“Œ Context
Case study-based certification tests require critical thinking evaluation, making full automation difficult. Some responses require human judgment to ensure correctness.

## ğŸ’¡ Decision
We will use a **Human-AI Hybrid Model**:
- AI performs the initial grading and assigns a **confidence score**.
- Responses with **low-confidence scores** are escalated to expert reviewers.
- Experts validate AI-graded responses and provide feedback to improve model accuracy.

## ğŸ›  Technologies
- **AI-Based Grading:** OpenAI GPT-4, Hugging Face Transformers
- **Human-in-the-Loop (HITL) Pipeline:** Label Studio, Amazon Augmented AI (A2I)
- **Confidence Score Mechanism:** AI flags responses needing review

## ğŸš€ Consequences
âœ… Balances automation and human expertise.  
âœ… Continuous AI model improvement via expert feedback.  
âœ… Ensures fairness in grading.  
â— Increases complexity in system integration.

## ğŸ“Œ Alternatives Considered
1ï¸âƒ£ **Fully AI-Driven Grading** â€“ Lacks human judgment, leading to potential misinterpretation. âŒ  
2ï¸âƒ£ **Fully Manual Grading** â€“ Slow and costly. âŒ  
3ï¸âƒ£ **Human-AI Hybrid Approach** â€“ Selected as the most efficient and fair solution. âœ…  


# ADR-003: Scalable Cloud Infrastructure for AI Grading System

## ğŸ“… Date
2025-02-16

## ğŸ¯ Status
âœ… Approved

## ğŸ“Œ Context
The grading system must handle **high-volume certification requests**, requiring **scalability and cost efficiency**.

## ğŸ’¡ Decision
We will deploy the AI grading models on a **cloud-native, serverless architecture**:
- **Compute:** Azure Machine Learning with AKS / AWS SageMaker with Lambda
- **Storage:** Vector databases (FAISS) for semantic similarity
- **Security:** Role-based access control (RBAC), encryption, audit logging

## ğŸ›  Technologies
- **Cloud Provider:** Azure / AWS / GCP
- **Serverless Execution:** Azure Functions, AWS Lambda
- **Containerization:** Kubernetes (AKS / EKS)

## ğŸš€ Consequences
âœ… Scales dynamically with demand.  
âœ… Reduces infrastructure cost via serverless execution.  
âœ… Ensures high availability and fault tolerance.  
â— Requires cloud cost monitoring and governance.

## ğŸ“Œ Alternatives Considered
1ï¸âƒ£ **On-Premises Deployment** â€“ High maintenance, not scalable. âŒ  
2ï¸âƒ£ **Traditional VMs** â€“ Inefficient and costly. âŒ  
3ï¸âƒ£ **Cloud-Native, Serverless Architecture** â€“ Best for scalability and cost efficiency. âœ…  


# ADR-004: Explainable AI (XAI) for Trust & Transparency in Grading

## ğŸ“… Date
2025-02-16

## ğŸ¯ Status
âœ… Approved

## ğŸ“Œ Context
Candidates and reviewers need visibility into AI-driven grading decisions to **ensure trust and fairness**.

## ğŸ’¡ Decision
We will implement **Explainable AI (XAI) techniques** to make AI grading decisions **auditable and interpretable**:
- **SHAP (SHapley Additive Explanations)** for feature importance analysis.
- **LIME (Local Interpretable Model-Agnostic Explanations)** to generate interpretable output.
- **Grading Breakdown Dashboard** for candidates to review score justifications.

## ğŸ›  Technologies
- **Explainability Frameworks:** SHAP, LIME
- **Dashboard Implementation:** React, Flask / FastAPI
- **Logging & Auditability:** ELK Stack (Elasticsearch, Logstash, Kibana)

## ğŸš€ Consequences
âœ… Builds trust with candidates and reviewers.  
âœ… Provides auditability for certification programs.  
âœ… Improves AI model interpretability.  
â— Adds computational overhead for XAI processing.

## ğŸ“Œ Alternatives Considered
1ï¸âƒ£ **No Explainability (Black Box AI)** â€“ Lacks transparency, reduces trust. âŒ  
2ï¸âƒ£ **Rule-Based Explainability** â€“ Too rigid for AI grading. âŒ  
3ï¸âƒ£ **XAI Techniques (SHAP, LIME, Dashboard)** â€“ Best for transparency and fairness. âœ…  

